### 实现一个大文件上传和断点续传

### 整体思路

核心利用`Blob.prototype.sice`方法和数组的 slice 方法累死，调用的 slice 方法可以返回原文件的某个切片
然后根据预先设置好的切片数量将文件切分为一个个切片，然后借助 http 的可并性(6 个)，同时上传切片，这样就从原来的大文件，变成了同时上传多个小的文件切片。

`注意`：由于是并发，传输到服务端的顺序可能会发生变化，所以我们还需要给每个切片记录顺序

### 问题延伸

- 上传多个文件时，如何保证上传完成的文件顺序？

当上传多个文件时，利用文件后边加顺序，去掉服务端然后根据约定的格式进行排序，假设中间出现断传，然后开启续传也可以填充进去，这样就保证了上传完成的文件顺序

- 上传大文件时，利用切片，如果保证上传完成的文件不会丢失？

- 上传大文件时，如何实现断点续传？

> 前端/服务端需要`记住`已上传的切片，这样下次上传就可以跳过之前已上传的部分

1.1 前端使用 localStorage 记录已上传的切片 hash
1.2 服务器报道已上传切片的 hash，前端每次上传前向服务端获取已上传的切片

因为换个浏览器所以存储的内容丢失了，所以采用方案二，那么这里有个`疑问`

- `断点续传的时候，已上传的时候，前端如何跳过呢请求呢？？`
  答：因为断点续传的时候，会向后端请求一个已上传文件的 hash,
  前端上传的时候，将已上传文件的 hash 过滤，因为前端发生文件时都会向后端发送一个 hahs，那么`疑问`这个 hash 是如何来的呢？可以借助`spark-md5`库去计算文件 hash，因为当内容不改动的时候，`hash`值是不会改变的，那么这样不就解决了 hash 的问题了吗？ 那么不就解决了断点续传时的文件丢失危险了吗？

- 暂停上传

使用原生的 abort 方法，axios 也是使用这个方法，当暂停上传的时候,包括当前的元素截取掉，保存到一个数组里边，等待二次恢复上传，上传成功一个，暂停数组里边删除一个

- 恢复上传

点击上传时，获取是否需要上传和已上传的切片

- 上传文件切片的核心是？

核心就是根据根据获取上传文件总大小，然后根据设置文件大小切割的大小去进行文件切割，切割的时候进行文件重命名

- 上传进度条

可以通过 onProgress 进行监听进度条，切片的话可以给每个进度条设置一个公共函数，上传完成即调用

### 总结：

- 大文件上传

  1.通过 Blob.prototype.slice 进行文件切片，并发上传多个切片，最后发送一个合并的请求通知后端进行合并切片

  2.后端接收切片并存储，收到合并请求后使用流将文件排序后将切片合并到最终的文件

  3.使用 xhr 原生 onProgress 进行上传进度的监听

- 断点续传

  1.使用`spark-md5`根据文件内容计算出 hash

  2.通过 hash 可以判断服务端是否已经上传该文件，如果已经上传，直接提示用户上传成功，从而实现秒传(假秒传)

  3.通过 xhr 的 abort 进行上传暂停，然后将已上传的文件从队列里边清除掉

  4.恢复上传的时候调用后端获取已上传文件的 hash，过滤已上传文件的 hash（因为只有当内容改变的时候 hash 才会改变），循环执行文件队列，然后开始上传切片

  5.后端开始切片排序然后通过流合并切片

### 参考链接

https://juejin.cn/post/6844904046436843527#heading-14

https://cdn.nlark.com/yuque/0/2020/svg/1134813/1609224978482-7d007435-dcc7-4fb2-8ea3-082e8ee49f9c.svg
